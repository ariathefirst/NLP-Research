{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "from scipy import spatial\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Model\n",
    "\n",
    "Our model was trained by op-eds scraped from Washington Post, CNN and Fox News on topic about Obama Care and Trump Care. So these are expert opinions on healthcare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_model = models.Word2Vec.load(\"our_model\")\n",
    "del our_model.wv.vocab['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Model\n",
    "\n",
    "we compare \"our model\" to the model trained with generic Google news data (more generic than our healthcare data)\n",
    "Download: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "Open file and rename it \"google.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.79399991035461"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "google = models.KeyedVectors.load_word2vec_format('google.bin',binary=True)\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build in features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most similar\n",
    "\n",
    "- This function is trying to find the similarity between words relationship. For example, the code:\n",
    "- \"our_model.most_similar(positive=['woman', 'female'], negative=['man'], topn=5)\"\n",
    "- explores the logical relationship of: 'woman' to 'man' is like 'female' to '___'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'latinos', 0.6830320358276367),\n",
       " (u'(including', 0.6655078530311584),\n",
       " (u'minorities', 0.6600582003593445),\n",
       " (u'non-voters', 0.6593189835548401),\n",
       " (u'male', 0.6519131064414978)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_model.most_similar(positive=['women', 'female'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'females', 0.6208751201629639),\n",
       " (u'Female', 0.5989526510238647),\n",
       " (u'Women', 0.5655556321144104),\n",
       " (u'womens', 0.5516568422317505),\n",
       " (u'male', 0.5251103639602661)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google.most_similar(positive=['women', 'female'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doesn't match\n",
    "\n",
    "- detects most dissimilar vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kevin'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_model.doesnt_match(\"man woman inhuman kevin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inhuman'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google.doesnt_match(\"man woman inhuman kevin\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5670666358568155"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_model.similarity('women', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5303777486481195"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google.similarity('woman', 'women')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we have come up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Similarity\n",
    "- calculates the average vector for all words in every sentence and use cosine similarity between vectors\n",
    "- the average vector of a given sentence is calculated by taking average on each element of the vector. This method doesn't take order of words in account. For example, \"This is a test\" should be identical with \"A test is this\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_feature_vector(sentence, model, num_features):\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4357183575630188"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_afv = avg_feature_vector('obamacare abortion preexisting coverage', model=our_model, num_features=50)\n",
    "s2_afv = avg_feature_vector('democrats', model=our_model, num_features=50)\n",
    "sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33329442143440247"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_afv = avg_feature_vector('obamacare abortion preexisting coverage', model=google, num_features=300)\n",
    "s2_afv = avg_feature_vector('democrats', model=google, num_features=300)\n",
    "sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\n",
    "sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for vector subtraction and addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Following the idea of Mikolov, this tests vector addtion and substraction on both our model and Google's model. The resulting vector is then compared to an expectation word. For example, following Mikolov, \"king\" - \"man\" + \"woman\" should be equal to \"queen\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3298827111721039"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difO = our_model['obamacare'] - our_model['democrats'] + our_model['republican']\n",
    "simO = 1 - spatial.distance.cosine(difO, our_model['healthcare'])\n",
    "simO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7300516963005066"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difG = google['king'] - google['man'] + google['woman']\n",
    "simG = 1 - spatial.distance.cosine(difG, google['queen'])\n",
    "simG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then we are wondering whether the result vector really means something, so we show the distance to the closest ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'obamacare', 0.771660566329956),\n",
       " (u'\\u201crepeal', 0.7148782014846802),\n",
       " (u'obamacare\\u201d', 0.7135400772094727),\n",
       " (u'republican', 0.7030623555183411),\n",
       " (u'\\u201cskinny', 0.6987356543540955),\n",
       " (u'repeal', 0.6965476870536804),\n",
       " (u\"senate's\", 0.6873770356178284),\n",
       " (u'sink', 0.6867955923080444),\n",
       " (u'obamacare)', 0.6823086738586426),\n",
       " (u'repeal\\u201d', 0.6795483231544495)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_model.most_similar(positive = [difO], topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'king', 0.8449392318725586),\n",
       " (u'queen', 0.7300517559051514),\n",
       " (u'monarch', 0.6454660892486572),\n",
       " (u'princess', 0.6156251430511475),\n",
       " (u'crown_prince', 0.5818676948547363),\n",
       " (u'prince', 0.5777117609977722),\n",
       " (u'kings', 0.5613663792610168),\n",
       " (u'sultan', 0.5376776456832886),\n",
       " (u'Queen_Consort', 0.5344247817993164),\n",
       " (u'queens', 0.5289887189865112)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google.most_similar(positive = [difG], topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def find_similar(model, dem, vec, topn):\n",
    "    waitlist = [k for (k,v) in model.wv.vocab.items()]\n",
    "    distance = []\n",
    "    for word in [a for a in waitlist]:\n",
    "        for d in range(2):\n",
    "            if abs(vec[d] - model.wv[word][d]) >= 0.2:\n",
    "                waitlist.remove(word)\n",
    "                break\n",
    "    print(len(waitlist))\n",
    "    for word in waitlist:\n",
    "        distance.append((word, spatial.distance.cosine(vec, model.wv[word])))\n",
    "    distance = sorted(distance, key = lambda k: k[1])[1:topn+1]\n",
    "    print(distance)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "[(u'marketplaces', 0.46691805124282837), (u'2010', 0.473321795463562), (u'current', 0.5403972268104553), (u'form', 0.5978847146034241), (u'himself', 0.6058657169342041)]\n"
     ]
    }
   ],
   "source": [
    "find_similar(our_model,50,difO,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models on Politician Related Problems\n",
    "\n",
    "- on politician related problems, which form most of our model's input, our model seems to perform better than Google's news model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('democrats', 0.7632335424423218),\n",
       " ('dems', 0.7000274062156677),\n",
       " ('repeal', 0.6927065253257751),\n",
       " ('filibuster', 0.6834850311279297),\n",
       " ('trumpcare', 0.6723067164421082)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_model.most_similar(positive=['obamacare', 'republicans'], negative=['abortion'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rethugs', 0.6481903195381165),\n",
       " ('dems', 0.6273144483566284),\n",
       " ('repubs', 0.621368408203125),\n",
       " ('repugs', 0.5910837650299072),\n",
       " ('rethugs', 0.578933596611023)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google.most_similar(positive=['obamacare', 'republicans'], negative=['abortion'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
